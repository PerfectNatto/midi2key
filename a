#include <iostream>
#include <memory>
#include <cstdlib>  // for malloc/free

#include <gflags/gflags.h>

#include <executorch/extension/data_loader/file_data_loader.h>
#include <executorch/extension/evalue_util/print_evalue.h>
#include <executorch/extension/runner_util/inputs.h>
#include <executorch/runtime/core/event_tracer.h>
#include <executorch/runtime/core/memory_allocator.h>  // MemoryAllocator for malloc-based pools
#include <executorch/runtime/executor/method.h>
#include <executorch/runtime/executor/program.h>
#include <executorch/runtime/platform/log.h>
#include <executorch/runtime/platform/platform.h>
#include <executorch/runtime/platform/runtime.h>
#ifdef ET_EVENT_TRACER_ENABLED
#include <executorch/devtools/etdump/etdump_flatcc.h>
#endif // ET_EVENT_TRACER_ENABLED

#if defined(ET_USE_THREADPOOL)
#include <executorch/extension/threadpool/cpuinfo_utils.h>
#include <executorch/extension/threadpool/threadpool.h>
#endif

DEFINE_string(
    model_path,
    "model.pte",
    "Model serialized in flatbuffer format.");
DEFINE_uint32(num_executions, 1, "Number of times to run the model.");
#ifdef ET_EVENT_TRACER_ENABLED
DEFINE_string(etdump_path, "model.etdump", "Write ETDump data to this path.");
#endif // ET_EVENT_TRACER_ENABLED
DEFINE_int32(
    cpu_threads,
    -1,
    "Number of CPU threads for inference. Defaults to -1, which implies we'll use a heuristic to derive the # of performant cores for a specific device.");

using executorch::extension::FileDataLoader;
using executorch::runtime::Error;
using executorch::runtime::EValue;
using executorch::runtime::EventTracer;
using executorch::runtime::HierarchicalAllocator;
using executorch::runtime::MemoryAllocator;  // malloc-based allocator
using executorch::runtime::MemoryManager;
using executorch::runtime::Method;
using executorch::runtime::MethodMeta;
using executorch::runtime::Program;
using executorch::runtime::Result;
using executorch::runtime::Span;

/// Helper to manage resources for ETDump generation
class EventTraceManager {
 public:
  EventTraceManager() : event_tracer_ptr_(nullptr) {
#ifdef ET_EVENT_TRACER_ENABLED
    event_tracer_ptr_ = std::make_shared<executorch::etdump::ETDumpGen>();
#endif
  }

  EventTracer* get_event_tracer() const {
    return event_tracer_ptr_.get();
  };

  Error write_etdump_to_file() const {
    EventTracer* const event_tracer_ptr = get_event_tracer();
    if (!event_tracer_ptr) {
      return Error::NotSupported;
    }

#ifdef ET_EVENT_TRACER_ENABLED
    executorch::etdump::ETDumpGen* const etdump_ptr =
        static_cast<executorch::etdump::ETDumpGen*>(event_tracer_ptr);

    const char* filename = FLAGS_etdump_path.c_str();

    std::unique_ptr<FILE, decltype(&fclose)> etdump_file(
        fopen(filename, "w+"), fclose);
    if (!etdump_file) {
      ET_LOG(Error, "Failed to open ETDump file at %s.", filename);
      return Error::AccessFailed;
    }

    executorch::etdump::ETDumpResult result = etdump_ptr->get_etdump_data();
    if (result.buf != nullptr && result.size > 0) {
      fwrite((uint8_t*)result.buf, 1, result.size, etdump_file.get());
      free(result.buf);
      ET_LOG(Info, "ETDump written to file '%s'.", filename);
    } else {
      ET_LOG(Error, "No ETDump data available!");
      return Error::NotFound;
    }
#endif

    return Error::Ok;
  }

 private:
  std::shared_ptr<EventTracer> event_tracer_ptr_;
};

int main(int argc, char** argv) {
  executorch::runtime::runtime_init();

  gflags::ParseCommandLineFlags(&argc, &argv, true);
  if (argc != 1) {
    std::string msg = "Extra commandline args:";
    for (int i = 1; i < argc; i++) {
      msg += std::string(" ") + argv[i];
    }
    ET_LOG(Error, "%s", msg.c_str());
    return 1;
  }

#if defined(ET_USE_THREADPOOL)
  auto cpu_threads = FLAGS_cpu_threads;
  uint32_t num_performant_cores = cpu_threads == -1
      ? ::executorch::extension::cpuinfo::get_num_performant_cores()
      : static_cast<uint32_t>(cpu_threads);
  ET_LOG(
      Info, "Resetting threadpool with num threads = %d", num_performant_cores);
  if (num_performant_cores > 0) {
    ::executorch::extension::threadpool::get_threadpool()
        ->_unsafe_reset_threadpool(num_performant_cores);
  }
#endif

  const char* model_path = FLAGS_model_path.c_str();
  Result<FileDataLoader> loader = FileDataLoader::from(model_path);
  ET_CHECK_MSG(
      loader.ok(),
      "FileDataLoader::from() failed: 0x%" PRIx32,
      (uint32_t)loader.error());

  Result<Program> program = Program::load(&loader.get());
  if (!program.ok()) {
    ET_LOG(Error, "Failed to parse model file %s", model_path);
    return 1;
  }
  ET_LOG(Info, "Model file %s is loaded.", model_path);

  const char* method_name = nullptr;
  {
    const auto method_name_result = program->get_method_name(0);
    ET_CHECK_MSG(method_name_result.ok(), "Program has no methods");
    method_name = *method_name_result;
  }
  ET_LOG(Info, "Using method %s", method_name);

  Result<MethodMeta> method_meta = program->method_meta(method_name);
  ET_CHECK_MSG(
      method_meta.ok(),
      "Failed to get method_meta for %s: 0x%" PRIx32,
      method_name,
      (uint32_t)method_meta.error());

  // Allocate method and temp pools dynamically
  size_t method_pool_size = method_meta->num_memory_planned_buffers() > 0
      ? static_cast<size_t>(method_meta->memory_planned_buffer_size(0).get()) * method_meta->num_memory_planned_buffers()
      : 1024*1024;
  uint8_t* method_pool = static_cast<uint8_t*>(std::malloc(method_pool_size));
  MemoryAllocator method_allocator(
      static_cast<uint32_t>(method_pool_size), method_pool);

  size_t temp_pool_size = method_pool_size;
  uint8_t* temp_pool = static_cast<uint8_t*>(std::malloc(temp_pool_size));
  MemoryAllocator temp_allocator(
      static_cast<uint32_t>(temp_pool_size), temp_pool);

  // Set up planned buffers
  std::vector<std::unique_ptr<uint8_t[]>> planned_buffers;
  std::vector<Span<uint8_t>> planned_spans;
  size_t num_mem = method_meta->num_memory_planned_buffers();
  for (size_t id = 0; id < num_mem; ++id) {
    size_t buffer_size =
        static_cast<size_t>(method_meta->memory_planned_buffer_size(id).get());
    ET_LOG(Info, "Setting up planned buffer %zu, size %zu.", id, buffer_size);
    planned_buffers.push_back(std::make_unique<uint8_t[]>(buffer_size));
    planned_spans.push_back({planned_buffers.back().get(), buffer_size});
  }
  HierarchicalAllocator planned_memory(
      {planned_spans.data(), planned_spans.size()});

  MemoryManager memory_manager(
      &method_allocator, &planned_memory, &temp_allocator);

  EventTraceManager tracer;
  Result<Method> method = program->load_method(
      method_name, &memory_manager, tracer.get_event_tracer());
  ET_CHECK_MSG(
      method.ok(),
      "Loading of method %s failed with status 0x%" PRIx32,
      method_name,
      (uint32_t)method.error());
  ET_LOG(Info, "Method loaded.");

  et_timestamp_t time_spent_executing = 0;
  for (uint32_t i = 0; i < FLAGS_num_executions; i++) {
    ET_LOG(Debug, "Preparing inputs.");
    auto inputs = executorch::extension::prepare_input_tensors(*method);
    ET_CHECK_MSG(
        inputs.ok(),
        "Could not prepare inputs: 0x%" PRIx32,
        (uint32_t)inputs.error());
    ET_LOG(Debug, "Inputs prepared.");

    const et_timestamp_t before_execute =
        executorch::runtime::pal_current_ticks();
    Error status = method->execute();
    const et_timestamp_t after_execute =
        executorch::runtime::pal_current_ticks();
    time_spent_executing += after_execute - before_execute;
    ET_CHECK_MSG(
        status == Error::Ok,
        "Execution of method %s failed with status 0x%" PRIx32,
        method_name,
        (uint32_t)status);
  }
  const auto tick_ratio = et_pal_ticks_to_ns_multiplier();
  constexpr auto NANOSECONDS_PER_MILLISECOND = 1000000;
  ET_LOG(
      Info,
      "Model executed successfully %u time(s) in %f ms.",
      FLAGS_num_executions,
      static_cast<double>(time_spent_executing) *
          tick_ratio.numerator /
          tick_ratio.denominator / NANOSECONDS_PER_MILLISECOND);

  std::vector<EValue> outputs(method->outputs_size());
  ET_LOG(Info, "%zu outputs:", outputs.size());
  Error status = method->get_outputs(outputs.data(), outputs.size());
  ET_CHECK(status == Error::Ok);
  std::cout << executorch::extension::evalue_edge_items(100);
  for (size_t i = 0; i < outputs.size(); ++i) {
    std::cout << "Output " << i << ": " << outputs[i] << std::endl;
  }

  if (tracer.get_event_tracer()) {
    status = tracer.write_etdump_to_file();
    ET_CHECK_MSG(status == Error::Ok, "Failed to save ETDump file.");
  }

  // Free dynamic pools
  std::free(method_pool);
  std::free(temp_pool);

  return 0;
}
