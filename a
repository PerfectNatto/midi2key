#include <jni.h>
#include "onnxruntime_cxx_api.h"

extern "C" JNIEXPORT void JNICALL
Java_com_example_app_MainActivity_runModel(JNIEnv* env, jobject /* this */) {
    // 環境・オプション初期化
    Ort::Env ort_env(ORT_LOGGING_LEVEL_WARNING, "onnx_native");
    Ort::SessionOptions session_options;
    session_options.SetIntraOpNumThreads(1);

    // モデル読み込み
    const char* model_path = "/data/user/0/com.example.app/files/model.onnx";
    Ort::Session session(ort_env, model_path, session_options);

    // 入力テンソル生成例
    auto memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);
    std::vector<float> input_vals = {/* … */};
    std::vector<int64_t> input_shape = {1, 3, 224, 224};
    Ort::Value input_tensor =
        Ort::Value::CreateTensor<float>(memory_info,
                                        input_vals.data(),
                                        input_vals.size(),
                                        input_shape.data(),
                                        input_shape.size());

    // 推論実行
    const char* input_names[]  = {"input"};
    const char* output_names[] = {"output"};
    auto output_tensors = session.Run(Ort::RunOptions{nullptr},
                                      input_names, &input_tensor, 1,
                                      output_names, 1);

    // 出力処理（例：最初の値を取得）
    float* out_data = output_tensors.front().GetTensorMutableData<float>();
    // … out_data[0] を利用 …
}
