import numpy as np
import onnxruntime as ort

# 1) セッションの作成
#    providers によってバックエンドを選択。GPU(CUDA)を使う場合は "CUDAExecutionProvider" を、
#    CPU のみなら "CPUExecutionProvider" を指定します。
session = ort.InferenceSession("model.onnx",
    providers=["CUDAExecutionProvider", "CPUExecutionProvider"]
)

# 2) 入力名・出力名の取得
input_name  = session.get_inputs()[0].name
output_name = session.get_outputs()[0].name

# 3) 入力データの用意 (例: 1x3x224x224 のランダムテンソル)
#    実際は前処理済みの NumPy 配列を渡します。
input_array = np.random.rand(1, 3, 224, 224).astype(np.float32)

# 4) 推論実行
#    run の第1引数には出力名のリスト、第2引数に {入力名: データ} の dict を与えます。
outputs = session.run([output_name], {input_name: input_array})

# 5) 結果の取得
pred = outputs[0]  # NumPy 配列で返ってきます
print(pred.shape)
